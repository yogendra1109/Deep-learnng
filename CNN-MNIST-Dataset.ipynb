{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The MNIST data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1,784)\n",
    "x_test = x_test.reshape(-1,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y_train = pd.get_dummies(y_train).values\n",
    "y_test = pd.get_dummies(y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 10), (10000, 10))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Initial parameters</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create general parameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 28 # width of the image in pixels \n",
    "height = 28 # height of the image in pixels\n",
    "flat = width * height # number of pixels in one image \n",
    "class_output = 10 # number of possible classifications for the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Input and output</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create place holders for inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    x  = tf.compat.v1.placeholder(tf.float32, shape=[None, flat])\n",
    "    y_ = tf.compat.v1.placeholder(tf.float32, shape=[None, class_output])\n",
    "    \n",
    "    x_image = tf.reshape(x, [-1,28,28,1])  \n",
    "\n",
    "    W_conv1 = tf.Variable(tf.compat.v1.truncated_normal([5, 5, 1, 32], stddev=0.1))\n",
    "    b_conv1 = tf.Variable(tf.constant(0.1, shape=[32])) # need 32 biases for 32 outputs\n",
    "\n",
    "    convolve1= tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1\n",
    "    \n",
    "    h_conv1 = tf.nn.relu(convolve1)\n",
    "    \n",
    "    conv1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding='SAME') #max_pool_2x2\n",
    "    \n",
    "    W_conv2 = tf.Variable(tf.compat.v1.truncated_normal([5, 5, 32, 64], stddev=0.1))\n",
    "    b_conv2 = tf.Variable(tf.constant(0.1, shape=[64])) #need 64 biases for 64 outputs\n",
    "    \n",
    "    convolve2= tf.nn.conv2d(conv1, W_conv2, strides=[1, 2, 2, 1], padding='SAME') + b_conv2\n",
    "    \n",
    "    h_conv2 = tf.nn.relu(convolve2)\n",
    "    \n",
    "    conv2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') #max_pool_2x2\n",
    "\n",
    "    layer2_matrix = tf.reshape(conv2, [-1, 7 * 7 * 64])\n",
    "    \n",
    "    W_fc1 = tf.Variable(tf.compat.v1.truncated_normal([7 * 7 * 64, 1024], stddev=0.1))\n",
    "    b_fc1 = tf.Variable(tf.constant(0.1, shape=[1024])) # need 1024 biases for 1024 outputs\n",
    "    \n",
    "    fcl = tf.matmul(layer2_matrix, W_fc1) + b_fc1\n",
    "    \n",
    "    h_fc1 = tf.nn.relu(fcl)\n",
    "    \n",
    "    keep_prob = tf.compat.v1.placeholder(tf.float32)\n",
    "    layer_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    W_fc2 = tf.Variable(tf.compat.v1.truncated_normal([1024, 10], stddev=0.1)) #1024 neurons\n",
    "    b_fc2 = tf.Variable(tf.constant(0.1, shape=[10])) # 10 possibilities for digits [0,1,2,3,4,5,6,7,8,9]\n",
    "    \n",
    "    fc=tf.matmul(layer_drop, W_fc2) + b_fc2\n",
    "    \n",
    "    y_CNN= tf.nn.softmax(fc)\n",
    "    \n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.compat.v1.log(y_CNN), axis=1))\n",
    "    \n",
    "    train_step = tf.compat.v1.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y_CNN, 1), tf.argmax(y_, 1))\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    init_op = tf.compat.v1.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session(graph=graph)\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.0958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 11/200 [02:41<44:10, 14.02s/it] "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "j=0\n",
    "for i in tqdm(range(200)):\n",
    "#     print(i)\n",
    "    if j > x_train.shape[0]:\n",
    "        j = 0\n",
    "    tr_x = x_train[j:j+5000]\n",
    "    tr_y = y_train[j:j+5000]\n",
    "    j+=5000\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={\n",
    "            x:tr_x, y_: tr_y, keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    sess.run(train_step, feed_dict={x: tr_x, y_: tr_y, keep_prob: 0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref9\"></a>\n",
    "<h2>Evaluate the model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the evaluation to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate in batches to avoid out-of-memory issues\n",
    "n_batches = mnist.test.images.shape[0] // 50\n",
    "cumulative_accuracy = 0.0\n",
    "for index in range(n_batches):\n",
    "    batch = mnist.test.next_batch(50)\n",
    "    cumulative_accuracy += accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "print(\"test accuracy {}\".format(cumulative_accuracy / n_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
