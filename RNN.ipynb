{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 5, 1) (12, 5, 1) (91,) (12,)\n",
      "Train on 91 samples, validate on 12 samples\n",
      "Epoch 1/350\n",
      "91/91 - 2s - loss: 157151375.9121 - mae: 10789.0820 - val_loss: 32502800.0000 - val_mae: 5157.7690\n",
      "Epoch 2/350\n",
      "91/91 - 0s - loss: 55405296.9670 - mae: 6295.3696 - val_loss: 135639088.0000 - val_mae: 10764.4551\n",
      "Epoch 3/350\n",
      "91/91 - 0s - loss: 79776372.3077 - mae: 7751.9629 - val_loss: 55156804.0000 - val_mae: 6267.1851\n",
      "Epoch 4/350\n",
      "91/91 - 0s - loss: 31306704.8736 - mae: 4491.9575 - val_loss: 16917426.0000 - val_mae: 2824.8105\n",
      "Epoch 5/350\n",
      "91/91 - 0s - loss: 20629280.7692 - mae: 3382.1733 - val_loss: 43926000.0000 - val_mae: 5548.6392\n",
      "Epoch 6/350\n",
      "91/91 - 0s - loss: 29731649.0549 - mae: 4469.4702 - val_loss: 39815036.0000 - val_mae: 5205.5503\n",
      "Epoch 7/350\n",
      "91/91 - 0s - loss: 21150885.8462 - mae: 3599.8528 - val_loss: 15326373.0000 - val_mae: 3364.8157\n",
      "Epoch 8/350\n",
      "91/91 - 0s - loss: 12251935.1648 - mae: 2687.4612 - val_loss: 13857949.0000 - val_mae: 2861.7324\n",
      "Epoch 9/350\n",
      "91/91 - 0s - loss: 15542568.8022 - mae: 3300.5220 - val_loss: 19291160.0000 - val_mae: 3396.3958\n",
      "Epoch 10/350\n",
      "91/91 - 0s - loss: 26972631.1648 - mae: 3928.3479 - val_loss: 14323749.0000 - val_mae: 3099.9656\n",
      "Epoch 11/350\n",
      "91/91 - 0s - loss: 26818726.0110 - mae: 3687.3215 - val_loss: 16861432.0000 - val_mae: 3688.8994\n",
      "Epoch 12/350\n",
      "91/91 - 0s - loss: 34121500.5714 - mae: 4201.3428 - val_loss: 67824160.0000 - val_mae: 6153.2319\n",
      "Epoch 13/350\n",
      "91/91 - 0s - loss: 40694497.2747 - mae: 4621.0469 - val_loss: 52806460.0000 - val_mae: 5182.6890\n",
      "Epoch 14/350\n",
      "91/91 - 0s - loss: 35221507.9560 - mae: 4450.4219 - val_loss: 40701120.0000 - val_mae: 4683.5352\n",
      "Epoch 15/350\n",
      "91/91 - 0s - loss: 27224197.3736 - mae: 4182.2842 - val_loss: 28256832.0000 - val_mae: 3768.6023\n",
      "Epoch 16/350\n",
      "91/91 - 0s - loss: 16149656.0989 - mae: 3198.6038 - val_loss: 24495552.0000 - val_mae: 3752.6116\n",
      "Epoch 17/350\n",
      "91/91 - 0s - loss: 10809309.5934 - mae: 2512.0161 - val_loss: 29722416.0000 - val_mae: 4518.9243\n",
      "Epoch 18/350\n",
      "91/91 - 0s - loss: 11839213.7143 - mae: 2596.6033 - val_loss: 18724026.0000 - val_mae: 3821.3757\n",
      "Epoch 19/350\n",
      "91/91 - 0s - loss: 11979349.8791 - mae: 2629.8594 - val_loss: 36883000.0000 - val_mae: 4875.5249\n",
      "Epoch 20/350\n",
      "91/91 - 0s - loss: 15368243.7582 - mae: 3075.0818 - val_loss: 24606970.0000 - val_mae: 3597.1504\n",
      "Epoch 21/350\n",
      "91/91 - 0s - loss: 15335347.0989 - mae: 3133.3572 - val_loss: 18569010.0000 - val_mae: 3572.0273\n",
      "Epoch 22/350\n",
      "91/91 - 0s - loss: 17409431.9780 - mae: 3245.0459 - val_loss: 14363731.0000 - val_mae: 3064.7258\n",
      "Epoch 23/350\n",
      "91/91 - 0s - loss: 13706862.7692 - mae: 2858.1416 - val_loss: 15001405.0000 - val_mae: 3096.9512\n",
      "Epoch 24/350\n",
      "91/91 - 0s - loss: 12697167.3956 - mae: 2811.3838 - val_loss: 17678736.0000 - val_mae: 3234.3416\n",
      "Epoch 25/350\n",
      "91/91 - 0s - loss: 11937832.6703 - mae: 2725.6638 - val_loss: 12920531.0000 - val_mae: 2704.2861\n",
      "Epoch 26/350\n",
      "91/91 - 0s - loss: 11495747.7582 - mae: 2690.6243 - val_loss: 13169944.0000 - val_mae: 2940.7781\n",
      "Epoch 27/350\n",
      "91/91 - 0s - loss: 10953798.2857 - mae: 2565.7695 - val_loss: 15492399.0000 - val_mae: 3216.6062\n",
      "Epoch 28/350\n",
      "91/91 - 0s - loss: 10571079.4505 - mae: 2541.6274 - val_loss: 14117147.0000 - val_mae: 3134.5654\n",
      "Epoch 29/350\n",
      "91/91 - 0s - loss: 9779947.4286 - mae: 2404.6191 - val_loss: 12905573.0000 - val_mae: 2831.3467\n",
      "Epoch 30/350\n",
      "91/91 - 0s - loss: 8954713.9615 - mae: 2343.9812 - val_loss: 10900325.0000 - val_mae: 2628.0164\n",
      "Epoch 31/350\n",
      "91/91 - 0s - loss: 9103752.0385 - mae: 2363.8452 - val_loss: 14315219.0000 - val_mae: 3134.3477\n",
      "Epoch 32/350\n",
      "91/91 - 0s - loss: 8849608.7802 - mae: 2317.9097 - val_loss: 14215820.0000 - val_mae: 3119.5430\n",
      "Epoch 33/350\n",
      "91/91 - 0s - loss: 13351959.1648 - mae: 2671.1006 - val_loss: 30907392.0000 - val_mae: 4010.6514\n",
      "Epoch 34/350\n",
      "91/91 - 0s - loss: 11993668.9890 - mae: 2523.5527 - val_loss: 12834680.0000 - val_mae: 3041.6760\n",
      "Epoch 35/350\n",
      "91/91 - 0s - loss: 9364190.1813 - mae: 2290.8533 - val_loss: 15885515.0000 - val_mae: 3344.2820\n",
      "Epoch 36/350\n",
      "91/91 - 0s - loss: 8307988.0989 - mae: 2183.2996 - val_loss: 13576415.0000 - val_mae: 3034.6794\n",
      "Epoch 37/350\n",
      "91/91 - 0s - loss: 8040924.5495 - mae: 2183.0310 - val_loss: 16835980.0000 - val_mae: 3055.5515\n",
      "Epoch 38/350\n",
      "91/91 - 0s - loss: 7673820.9505 - mae: 2094.7488 - val_loss: 15924745.0000 - val_mae: 2998.6062\n",
      "Epoch 39/350\n",
      "91/91 - 0s - loss: 7680523.4451 - mae: 2125.6575 - val_loss: 15221203.0000 - val_mae: 3041.2590\n",
      "Epoch 40/350\n",
      "91/91 - 0s - loss: 7338845.4341 - mae: 2067.6394 - val_loss: 14607451.0000 - val_mae: 2989.8621\n",
      "Epoch 41/350\n",
      "91/91 - 0s - loss: 6954039.4396 - mae: 2015.8230 - val_loss: 14102991.0000 - val_mae: 2990.2351\n",
      "Epoch 42/350\n",
      "91/91 - 0s - loss: 6670605.4945 - mae: 1977.1829 - val_loss: 13774787.0000 - val_mae: 2969.7830\n",
      "Epoch 43/350\n",
      "91/91 - 0s - loss: 6964491.4231 - mae: 1983.0621 - val_loss: 13391273.0000 - val_mae: 2933.0439\n",
      "Epoch 44/350\n",
      "91/91 - 0s - loss: 6711641.4615 - mae: 1933.3661 - val_loss: 13156035.0000 - val_mae: 2839.7073\n",
      "Epoch 45/350\n",
      "91/91 - 0s - loss: 6555713.0110 - mae: 1897.8403 - val_loss: 12783824.0000 - val_mae: 2814.3713\n",
      "Epoch 46/350\n",
      "91/91 - 0s - loss: 6396232.8132 - mae: 1877.4122 - val_loss: 12482421.0000 - val_mae: 2811.9880\n",
      "Epoch 47/350\n",
      "91/91 - 0s - loss: 6282602.4890 - mae: 1857.6591 - val_loss: 12286605.0000 - val_mae: 2778.4075\n",
      "Epoch 48/350\n",
      "91/91 - 0s - loss: 6318277.9176 - mae: 1865.1824 - val_loss: 12070141.0000 - val_mae: 2746.2412\n",
      "Epoch 49/350\n",
      "91/91 - 0s - loss: 6879120.0385 - mae: 1893.6219 - val_loss: 12248325.0000 - val_mae: 2755.7219\n",
      "Epoch 50/350\n",
      "91/91 - 0s - loss: 6770957.6648 - mae: 1961.8125 - val_loss: 12115661.0000 - val_mae: 2759.6404\n",
      "Epoch 51/350\n",
      "91/91 - 0s - loss: 6788176.5879 - mae: 2074.4858 - val_loss: 10846327.0000 - val_mae: 2790.7390\n",
      "Epoch 52/350\n",
      "91/91 - 0s - loss: 7415842.8571 - mae: 2175.8818 - val_loss: 18287824.0000 - val_mae: 3395.8235\n",
      "Epoch 53/350\n",
      "91/91 - 0s - loss: 8086237.2857 - mae: 2231.2498 - val_loss: 17908122.0000 - val_mae: 3176.8171\n",
      "Epoch 54/350\n",
      "91/91 - 0s - loss: 9440561.7473 - mae: 2393.3657 - val_loss: 17760432.0000 - val_mae: 3242.5818\n",
      "Epoch 55/350\n",
      "91/91 - 0s - loss: 9771961.4890 - mae: 2348.4766 - val_loss: 16054955.0000 - val_mae: 3329.0686\n",
      "Epoch 56/350\n",
      "91/91 - 0s - loss: 8783173.1538 - mae: 2207.7810 - val_loss: 13452427.0000 - val_mae: 3167.0579\n",
      "Epoch 57/350\n",
      "91/91 - 0s - loss: 8440119.7418 - mae: 2193.6240 - val_loss: 15931021.0000 - val_mae: 3523.0486\n",
      "Epoch 58/350\n",
      "91/91 - 0s - loss: 8776421.5604 - mae: 2243.3259 - val_loss: 16300547.0000 - val_mae: 3573.6731\n",
      "Epoch 59/350\n",
      "91/91 - 0s - loss: 8494419.9011 - mae: 2198.7068 - val_loss: 15300685.0000 - val_mae: 3367.8025\n",
      "Epoch 60/350\n",
      "91/91 - 0s - loss: 8518214.3462 - mae: 2212.0237 - val_loss: 11846741.0000 - val_mae: 3005.5391\n",
      "Epoch 61/350\n",
      "91/91 - 0s - loss: 8164450.1703 - mae: 2141.1658 - val_loss: 12628292.0000 - val_mae: 3154.2683\n",
      "Epoch 62/350\n",
      "91/91 - 0s - loss: 7881452.4176 - mae: 2110.3884 - val_loss: 12884364.0000 - val_mae: 3169.1846\n",
      "Epoch 63/350\n",
      "91/91 - 0s - loss: 7871871.1758 - mae: 2144.2820 - val_loss: 12981600.0000 - val_mae: 3129.4890\n",
      "Epoch 64/350\n",
      "91/91 - 0s - loss: 7823246.5495 - mae: 2133.2937 - val_loss: 11717485.0000 - val_mae: 2906.3408\n",
      "Epoch 65/350\n",
      "91/91 - 0s - loss: 7817229.0055 - mae: 2142.9282 - val_loss: 12015024.0000 - val_mae: 2916.7537\n",
      "Epoch 66/350\n",
      "91/91 - 0s - loss: 7575857.9890 - mae: 2102.7476 - val_loss: 12075817.0000 - val_mae: 2915.4514\n",
      "Epoch 67/350\n",
      "91/91 - 0s - loss: 7646442.0440 - mae: 2105.5356 - val_loss: 12089585.0000 - val_mae: 2901.8711\n",
      "Epoch 68/350\n",
      "91/91 - 0s - loss: 7598336.1429 - mae: 2109.5994 - val_loss: 12047676.0000 - val_mae: 2909.7520\n",
      "Epoch 69/350\n",
      "91/91 - 0s - loss: 7565447.9231 - mae: 2110.2849 - val_loss: 12823167.0000 - val_mae: 3099.8958\n",
      "Epoch 70/350\n",
      "91/91 - 0s - loss: 7563331.0385 - mae: 2133.8206 - val_loss: 12891347.0000 - val_mae: 3111.9065\n",
      "Epoch 71/350\n",
      "91/91 - 0s - loss: 7404232.0879 - mae: 2114.0442 - val_loss: 12782525.0000 - val_mae: 3054.5750\n",
      "Epoch 72/350\n",
      "91/91 - 0s - loss: 7405429.6703 - mae: 2104.4111 - val_loss: 12768089.0000 - val_mae: 3058.9377\n",
      "Epoch 73/350\n",
      "91/91 - 0s - loss: 7561783.5440 - mae: 2145.6731 - val_loss: 12748040.0000 - val_mae: 3090.8699\n",
      "Epoch 74/350\n",
      "91/91 - 0s - loss: 7562166.9670 - mae: 2148.5762 - val_loss: 12678157.0000 - val_mae: 3088.8347\n",
      "Epoch 75/350\n",
      "91/91 - 0s - loss: 7536510.7692 - mae: 2146.8398 - val_loss: 12632043.0000 - val_mae: 3055.4309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/350\n",
      "91/91 - 0s - loss: 7502558.2857 - mae: 2131.7083 - val_loss: 12632343.0000 - val_mae: 3037.0547\n",
      "Epoch 77/350\n",
      "91/91 - 0s - loss: 7526777.0440 - mae: 2120.6660 - val_loss: 12626880.0000 - val_mae: 3058.9534\n",
      "Epoch 78/350\n",
      "91/91 - 0s - loss: 7406266.0714 - mae: 2113.9099 - val_loss: 12692459.0000 - val_mae: 3067.0293\n",
      "Epoch 79/350\n",
      "91/91 - 0s - loss: 7362680.5714 - mae: 2115.5994 - val_loss: 12699856.0000 - val_mae: 3084.6985\n",
      "Epoch 80/350\n",
      "91/91 - 0s - loss: 7304564.1758 - mae: 2112.3154 - val_loss: 12776300.0000 - val_mae: 3102.7273\n",
      "Epoch 81/350\n",
      "91/91 - 0s - loss: 7306565.9560 - mae: 2115.7046 - val_loss: 12786965.0000 - val_mae: 3091.9519\n",
      "Epoch 82/350\n",
      "91/91 - 0s - loss: 7262106.9890 - mae: 2110.1392 - val_loss: 12852203.0000 - val_mae: 3111.8972\n",
      "Epoch 83/350\n",
      "91/91 - 0s - loss: 7245854.7692 - mae: 2117.6982 - val_loss: 12931108.0000 - val_mae: 3111.5312\n",
      "Epoch 84/350\n",
      "91/91 - 0s - loss: 7278276.8791 - mae: 2100.4490 - val_loss: 12892984.0000 - val_mae: 3067.7512\n",
      "Epoch 85/350\n",
      "91/91 - 0s - loss: 7318863.2198 - mae: 2079.4548 - val_loss: 12925911.0000 - val_mae: 3004.5642\n",
      "Epoch 86/350\n",
      "91/91 - 0s - loss: 7312931.3077 - mae: 2067.9287 - val_loss: 12952029.0000 - val_mae: 3029.0071\n",
      "Epoch 87/350\n",
      "91/91 - 0s - loss: 7515053.0165 - mae: 2146.4395 - val_loss: 12958363.0000 - val_mae: 3031.8447\n",
      "Epoch 88/350\n",
      "91/91 - 0s - loss: 7234938.2198 - mae: 2110.3752 - val_loss: 12836805.0000 - val_mae: 2977.8289\n",
      "Epoch 89/350\n",
      "91/91 - 0s - loss: 7150577.4396 - mae: 2073.4390 - val_loss: 12744161.0000 - val_mae: 2913.4993\n",
      "Epoch 90/350\n",
      "91/91 - 0s - loss: 7051127.6484 - mae: 2058.5049 - val_loss: 12738083.0000 - val_mae: 2908.8079\n",
      "Epoch 91/350\n",
      "91/91 - 0s - loss: 7087786.8846 - mae: 2087.9639 - val_loss: 12769003.0000 - val_mae: 2985.7910\n",
      "Epoch 92/350\n",
      "91/91 - 0s - loss: 7030165.4121 - mae: 2089.7290 - val_loss: 12503115.0000 - val_mae: 2888.8528\n",
      "Epoch 93/350\n",
      "91/91 - 0s - loss: 6993753.4890 - mae: 2091.3389 - val_loss: 12533612.0000 - val_mae: 2893.5332\n",
      "Epoch 94/350\n",
      "91/91 - 0s - loss: 6990570.3846 - mae: 2082.7395 - val_loss: 12434404.0000 - val_mae: 2896.3333\n",
      "Epoch 95/350\n",
      "91/91 - 0s - loss: 7050854.1978 - mae: 2070.9285 - val_loss: 12386231.0000 - val_mae: 2889.5852\n",
      "Epoch 96/350\n",
      "91/91 - 0s - loss: 6985856.8242 - mae: 2060.7856 - val_loss: 12474416.0000 - val_mae: 2883.9226\n",
      "Epoch 97/350\n",
      "91/91 - 0s - loss: 6941290.0659 - mae: 2058.7566 - val_loss: 12511948.0000 - val_mae: 2880.1633\n",
      "Epoch 98/350\n",
      "91/91 - 0s - loss: 6902902.2033 - mae: 2047.2455 - val_loss: 12541516.0000 - val_mae: 2876.0833\n",
      "Epoch 99/350\n",
      "91/91 - 0s - loss: 6868441.3187 - mae: 2056.4690 - val_loss: 12477399.0000 - val_mae: 2890.0085\n",
      "Epoch 100/350\n",
      "91/91 - 0s - loss: 6782152.7692 - mae: 2038.2655 - val_loss: 12441717.0000 - val_mae: 2887.4431\n",
      "Epoch 101/350\n",
      "91/91 - 0s - loss: 6777044.0549 - mae: 2040.1035 - val_loss: 12385877.0000 - val_mae: 2887.9929\n",
      "Epoch 102/350\n",
      "91/91 - 0s - loss: 6771381.8462 - mae: 2040.8987 - val_loss: 14173445.0000 - val_mae: 3096.0222\n",
      "Epoch 103/350\n",
      "91/91 - 0s - loss: 6807637.8187 - mae: 2050.6345 - val_loss: 14294187.0000 - val_mae: 3102.6863\n",
      "Epoch 104/350\n",
      "91/91 - 0s - loss: 6801412.1813 - mae: 2048.5449 - val_loss: 14261387.0000 - val_mae: 3118.8010\n",
      "Epoch 105/350\n",
      "91/91 - 0s - loss: 6751869.5000 - mae: 2023.7030 - val_loss: 14333147.0000 - val_mae: 3118.3489\n",
      "Epoch 106/350\n",
      "91/91 - 0s - loss: 6663948.2747 - mae: 2015.9338 - val_loss: 14487828.0000 - val_mae: 3122.5349\n",
      "Epoch 107/350\n",
      "91/91 - 0s - loss: 6758119.2857 - mae: 2024.5144 - val_loss: 14476471.0000 - val_mae: 3118.4148\n",
      "Epoch 108/350\n",
      "91/91 - 0s - loss: 6766217.6978 - mae: 2025.3800 - val_loss: 14484424.0000 - val_mae: 3121.4709\n",
      "Epoch 109/350\n",
      "91/91 - 0s - loss: 6997329.6813 - mae: 2048.7390 - val_loss: 14348225.0000 - val_mae: 3130.0000\n",
      "Epoch 110/350\n",
      "91/91 - 0s - loss: 6770264.1429 - mae: 2007.6080 - val_loss: 14367696.0000 - val_mae: 3121.5391\n",
      "Epoch 111/350\n",
      "91/91 - 0s - loss: 6745845.6813 - mae: 2013.1075 - val_loss: 14523692.0000 - val_mae: 3118.6865\n",
      "Epoch 112/350\n",
      "91/91 - 0s - loss: 6788715.0110 - mae: 2034.8916 - val_loss: 14689571.0000 - val_mae: 3134.6035\n",
      "Epoch 113/350\n",
      "91/91 - 0s - loss: 6736460.8297 - mae: 2028.2783 - val_loss: 14364680.0000 - val_mae: 3118.6455\n",
      "Epoch 114/350\n",
      "91/91 - 0s - loss: 6803122.7967 - mae: 2022.0658 - val_loss: 14334229.0000 - val_mae: 3129.7598\n",
      "Epoch 115/350\n",
      "91/91 - 0s - loss: 6771254.3132 - mae: 2016.0100 - val_loss: 14257272.0000 - val_mae: 3118.2009\n",
      "Epoch 116/350\n",
      "91/91 - 0s - loss: 6681052.7912 - mae: 2014.9011 - val_loss: 14271052.0000 - val_mae: 3116.5847\n",
      "Epoch 117/350\n",
      "91/91 - 0s - loss: 6635888.8791 - mae: 2007.7271 - val_loss: 14235872.0000 - val_mae: 3104.1614\n",
      "Epoch 118/350\n",
      "91/91 - 0s - loss: 6621193.0989 - mae: 2005.1133 - val_loss: 14095104.0000 - val_mae: 3093.5134\n",
      "Epoch 119/350\n",
      "91/91 - 0s - loss: 6616269.4231 - mae: 1995.4995 - val_loss: 14049827.0000 - val_mae: 3084.7439\n",
      "Epoch 120/350\n",
      "91/91 - 0s - loss: 6775104.8132 - mae: 2012.1292 - val_loss: 13932488.0000 - val_mae: 3088.5505\n",
      "Epoch 121/350\n",
      "91/91 - 0s - loss: 6476922.2473 - mae: 1956.8890 - val_loss: 14128976.0000 - val_mae: 3129.7180\n",
      "Epoch 122/350\n",
      "91/91 - 0s - loss: 6855883.5055 - mae: 2034.6619 - val_loss: 14728467.0000 - val_mae: 3258.8748\n",
      "Epoch 123/350\n",
      "91/91 - 0s - loss: 6806751.9918 - mae: 2036.0292 - val_loss: 14686029.0000 - val_mae: 3275.2090\n",
      "Epoch 124/350\n",
      "91/91 - 0s - loss: 6710315.1099 - mae: 2011.0841 - val_loss: 14697512.0000 - val_mae: 3249.7754\n",
      "Epoch 125/350\n",
      "91/91 - 0s - loss: 6734492.4560 - mae: 2009.6923 - val_loss: 14661763.0000 - val_mae: 3224.0979\n",
      "Epoch 126/350\n",
      "91/91 - 0s - loss: 6791598.9176 - mae: 2019.0076 - val_loss: 14630979.0000 - val_mae: 3255.7917\n",
      "Epoch 127/350\n",
      "91/91 - 0s - loss: 6691687.4945 - mae: 2005.7233 - val_loss: 14528901.0000 - val_mae: 3256.6956\n",
      "Epoch 128/350\n",
      "91/91 - 0s - loss: 6659908.9341 - mae: 2003.3995 - val_loss: 14501825.0000 - val_mae: 3206.3176\n",
      "Epoch 129/350\n",
      "91/91 - 0s - loss: 6635196.8681 - mae: 1995.7014 - val_loss: 14443493.0000 - val_mae: 3214.2246\n",
      "Epoch 130/350\n",
      "91/91 - 0s - loss: 6625517.7473 - mae: 1998.6154 - val_loss: 14136477.0000 - val_mae: 3066.0813\n",
      "Epoch 131/350\n",
      "91/91 - 0s - loss: 6687175.6374 - mae: 2011.7834 - val_loss: 14053248.0000 - val_mae: 3069.9541\n",
      "Epoch 132/350\n",
      "91/91 - 0s - loss: 6482628.2857 - mae: 1993.2610 - val_loss: 13966367.0000 - val_mae: 3058.7188\n",
      "Epoch 133/350\n",
      "91/91 - 0s - loss: 6603185.0110 - mae: 2017.4211 - val_loss: 13822588.0000 - val_mae: 3089.6633\n",
      "Epoch 134/350\n",
      "91/91 - 0s - loss: 6526667.0934 - mae: 1996.2737 - val_loss: 13906013.0000 - val_mae: 3074.7305\n",
      "Epoch 135/350\n",
      "91/91 - 0s - loss: 6496277.8791 - mae: 1992.4034 - val_loss: 13954287.0000 - val_mae: 3075.7617\n",
      "Epoch 136/350\n",
      "91/91 - 0s - loss: 6514610.0879 - mae: 1990.7124 - val_loss: 14339861.0000 - val_mae: 3148.6145\n",
      "Epoch 137/350\n",
      "91/91 - 0s - loss: 6503762.0934 - mae: 2001.2510 - val_loss: 14229396.0000 - val_mae: 3152.0105\n",
      "Epoch 138/350\n",
      "91/91 - 0s - loss: 6432548.3791 - mae: 1961.6570 - val_loss: 14215443.0000 - val_mae: 3174.3574\n",
      "Epoch 139/350\n",
      "91/91 - 0s - loss: 6400131.2088 - mae: 1960.0884 - val_loss: 14224736.0000 - val_mae: 3148.5918\n",
      "Epoch 140/350\n",
      "91/91 - 0s - loss: 6378548.0275 - mae: 1968.0341 - val_loss: 14481245.0000 - val_mae: 3163.7871\n",
      "Epoch 141/350\n",
      "91/91 - 0s - loss: 6360141.2967 - mae: 1962.2087 - val_loss: 14507136.0000 - val_mae: 3193.1985\n",
      "Epoch 142/350\n",
      "91/91 - 0s - loss: 6283282.4890 - mae: 1942.2517 - val_loss: 12859055.0000 - val_mae: 2942.5540\n",
      "Epoch 143/350\n",
      "91/91 - 0s - loss: 6360968.8571 - mae: 1939.8197 - val_loss: 13014685.0000 - val_mae: 2968.7686\n",
      "Epoch 144/350\n",
      "91/91 - 0s - loss: 6331701.0330 - mae: 1956.9988 - val_loss: 12720568.0000 - val_mae: 2916.3201\n",
      "Epoch 145/350\n",
      "91/91 - 0s - loss: 6294241.6923 - mae: 1971.9886 - val_loss: 12768947.0000 - val_mae: 2922.1243\n",
      "Epoch 146/350\n",
      "91/91 - 0s - loss: 6270012.0714 - mae: 1967.3898 - val_loss: 12996293.0000 - val_mae: 2959.0566\n",
      "Epoch 147/350\n",
      "91/91 - 0s - loss: 6291290.3681 - mae: 1955.0485 - val_loss: 12926605.0000 - val_mae: 2970.3359\n",
      "Epoch 148/350\n",
      "91/91 - 0s - loss: 6195317.0659 - mae: 1939.7073 - val_loss: 12800712.0000 - val_mae: 2957.3967\n",
      "Epoch 149/350\n",
      "91/91 - 0s - loss: 6247268.1044 - mae: 1949.6650 - val_loss: 12766408.0000 - val_mae: 2956.9167\n",
      "Epoch 150/350\n",
      "91/91 - 0s - loss: 6287961.8132 - mae: 1955.6494 - val_loss: 12692292.0000 - val_mae: 2943.5090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/350\n",
      "91/91 - 0s - loss: 6111114.0000 - mae: 1907.6791 - val_loss: 13186399.0000 - val_mae: 3071.9211\n",
      "Epoch 152/350\n",
      "91/91 - 0s - loss: 6312285.0165 - mae: 1947.8636 - val_loss: 12993301.0000 - val_mae: 3008.2083\n",
      "Epoch 153/350\n",
      "91/91 - 0s - loss: 6306903.8901 - mae: 1963.3159 - val_loss: 12511552.0000 - val_mae: 2933.6550\n",
      "Epoch 154/350\n",
      "91/91 - 0s - loss: 6334988.7527 - mae: 1972.9396 - val_loss: 12459027.0000 - val_mae: 2931.0833\n",
      "Epoch 155/350\n",
      "91/91 - 0s - loss: 6148260.2912 - mae: 1929.5560 - val_loss: 12708371.0000 - val_mae: 2962.9524\n",
      "Epoch 156/350\n",
      "91/91 - 0s - loss: 6193943.6264 - mae: 1931.7362 - val_loss: 12843843.0000 - val_mae: 3009.6804\n",
      "Epoch 157/350\n",
      "91/91 - 0s - loss: 6241323.0055 - mae: 1939.6249 - val_loss: 12576227.0000 - val_mae: 2936.4172\n",
      "Epoch 158/350\n",
      "91/91 - 0s - loss: 6147215.8681 - mae: 1926.6744 - val_loss: 12693059.0000 - val_mae: 2950.1399\n",
      "Epoch 159/350\n",
      "91/91 - 0s - loss: 6235600.5165 - mae: 1943.9015 - val_loss: 12885059.0000 - val_mae: 2994.9336\n",
      "Epoch 160/350\n",
      "91/91 - 0s - loss: 6109057.4780 - mae: 1928.6300 - val_loss: 12578275.0000 - val_mae: 2930.8555\n",
      "Epoch 161/350\n",
      "91/91 - 0s - loss: 6137376.3297 - mae: 1939.2655 - val_loss: 12529744.0000 - val_mae: 2924.8350\n",
      "Epoch 162/350\n",
      "91/91 - 0s - loss: 6045307.0412 - mae: 1930.5109 - val_loss: 12663563.0000 - val_mae: 2955.4001\n",
      "Epoch 163/350\n",
      "91/91 - 0s - loss: 6135154.5824 - mae: 1942.1707 - val_loss: 12739819.0000 - val_mae: 3008.9485\n",
      "Epoch 164/350\n",
      "91/91 - 0s - loss: 6213472.8901 - mae: 1946.3523 - val_loss: 12706591.0000 - val_mae: 3008.4590\n",
      "Epoch 165/350\n",
      "91/91 - 0s - loss: 5994754.4011 - mae: 1906.6735 - val_loss: 12496712.0000 - val_mae: 2931.0046\n",
      "Epoch 166/350\n",
      "91/91 - 0s - loss: 6045870.7390 - mae: 1925.8225 - val_loss: 12426952.0000 - val_mae: 2924.2722\n",
      "Epoch 167/350\n",
      "91/91 - 0s - loss: 6070727.4231 - mae: 1933.2736 - val_loss: 12423203.0000 - val_mae: 2909.4866\n",
      "Epoch 168/350\n",
      "91/91 - 0s - loss: 6093380.1593 - mae: 1916.6293 - val_loss: 12872139.0000 - val_mae: 3029.2871\n",
      "Epoch 169/350\n",
      "91/91 - 0s - loss: 6054617.2143 - mae: 1910.9017 - val_loss: 12493245.0000 - val_mae: 2938.1917\n",
      "Epoch 170/350\n",
      "91/91 - 0s - loss: 5957481.1319 - mae: 1900.7212 - val_loss: 12311492.0000 - val_mae: 2918.0771\n",
      "Epoch 171/350\n",
      "91/91 - 0s - loss: 6036089.5110 - mae: 1916.6627 - val_loss: 12338715.0000 - val_mae: 2920.9231\n",
      "Epoch 172/350\n",
      "91/91 - 0s - loss: 5964820.4396 - mae: 1891.6405 - val_loss: 12628040.0000 - val_mae: 2990.3538\n",
      "Epoch 173/350\n",
      "91/91 - 0s - loss: 6026354.1429 - mae: 1918.2074 - val_loss: 12432456.0000 - val_mae: 2931.0579\n",
      "Epoch 174/350\n",
      "91/91 - 0s - loss: 5969821.3407 - mae: 1900.6276 - val_loss: 12512347.0000 - val_mae: 2951.3594\n",
      "Epoch 175/350\n",
      "91/91 - 0s - loss: 5938858.3462 - mae: 1895.3398 - val_loss: 12343933.0000 - val_mae: 2935.5312\n",
      "Epoch 176/350\n",
      "91/91 - 0s - loss: 5880182.7363 - mae: 1888.7926 - val_loss: 12172935.0000 - val_mae: 2903.8005\n",
      "Epoch 177/350\n",
      "91/91 - 0s - loss: 5999794.7198 - mae: 1905.5427 - val_loss: 12278244.0000 - val_mae: 2944.9717\n",
      "Epoch 178/350\n",
      "91/91 - 0s - loss: 5962821.9615 - mae: 1889.4220 - val_loss: 12854188.0000 - val_mae: 3123.3699\n",
      "Epoch 179/350\n",
      "91/91 - 0s - loss: 5903312.5275 - mae: 1892.3809 - val_loss: 12638395.0000 - val_mae: 3039.0811\n",
      "Epoch 180/350\n",
      "91/91 - 0s - loss: 5911232.9560 - mae: 1891.5239 - val_loss: 12267859.0000 - val_mae: 2958.2126\n",
      "Epoch 181/350\n",
      "91/91 - 0s - loss: 5874294.6429 - mae: 1887.5295 - val_loss: 12424605.0000 - val_mae: 2969.7305\n",
      "Epoch 182/350\n",
      "91/91 - 0s - loss: 5910940.2473 - mae: 1893.1317 - val_loss: 12501136.0000 - val_mae: 3011.4875\n",
      "Epoch 183/350\n",
      "91/91 - 0s - loss: 5839191.1456 - mae: 1877.3970 - val_loss: 12856872.0000 - val_mae: 3068.6487\n",
      "Epoch 184/350\n",
      "91/91 - 0s - loss: 5970799.8516 - mae: 1911.5536 - val_loss: 13154104.0000 - val_mae: 3182.1453\n",
      "Epoch 185/350\n",
      "91/91 - 0s - loss: 5893835.2198 - mae: 1905.6876 - val_loss: 12489559.0000 - val_mae: 3048.6472\n",
      "Epoch 186/350\n",
      "91/91 - 0s - loss: 6133303.1484 - mae: 1946.1465 - val_loss: 12220881.0000 - val_mae: 2941.6946\n",
      "Epoch 187/350\n",
      "91/91 - 0s - loss: 6039780.2692 - mae: 1942.9222 - val_loss: 12552652.0000 - val_mae: 3047.4294\n",
      "Epoch 188/350\n",
      "91/91 - 0s - loss: 6035756.4505 - mae: 1906.5698 - val_loss: 13774573.0000 - val_mae: 3271.0535\n",
      "Epoch 189/350\n",
      "91/91 - 0s - loss: 5995934.6319 - mae: 1908.3053 - val_loss: 12473684.0000 - val_mae: 3022.9197\n",
      "Epoch 190/350\n",
      "91/91 - 0s - loss: 5886778.1484 - mae: 1872.0664 - val_loss: 11972195.0000 - val_mae: 2868.4387\n",
      "Epoch 191/350\n",
      "91/91 - 0s - loss: 5968607.8901 - mae: 1918.9604 - val_loss: 14350312.0000 - val_mae: 3273.0393\n",
      "Epoch 192/350\n",
      "91/91 - 0s - loss: 5834058.7308 - mae: 1879.9110 - val_loss: 14039503.0000 - val_mae: 3347.4160\n",
      "Epoch 193/350\n",
      "91/91 - 0s - loss: 5767713.8736 - mae: 1872.6779 - val_loss: 13863811.0000 - val_mae: 3235.6711\n",
      "Epoch 194/350\n",
      "91/91 - 0s - loss: 5711668.8736 - mae: 1871.9688 - val_loss: 13892667.0000 - val_mae: 3211.9238\n",
      "Epoch 195/350\n",
      "91/91 - 0s - loss: 5844471.6264 - mae: 1895.9585 - val_loss: 13921333.0000 - val_mae: 3218.0840\n",
      "Epoch 196/350\n",
      "91/91 - 0s - loss: 5659105.7308 - mae: 1848.5599 - val_loss: 13729744.0000 - val_mae: 3231.3594\n",
      "Epoch 197/350\n",
      "91/91 - 0s - loss: 5742580.2473 - mae: 1862.2864 - val_loss: 13626316.0000 - val_mae: 3214.2556\n",
      "Epoch 198/350\n",
      "91/91 - 0s - loss: 5683071.7912 - mae: 1853.7209 - val_loss: 13539332.0000 - val_mae: 3223.8438\n",
      "Epoch 199/350\n",
      "91/91 - 0s - loss: 5680392.7363 - mae: 1846.9189 - val_loss: 13714419.0000 - val_mae: 3197.5750\n",
      "Epoch 200/350\n",
      "91/91 - 0s - loss: 5640251.4011 - mae: 1838.4492 - val_loss: 13858480.0000 - val_mae: 3222.9270\n",
      "Epoch 201/350\n",
      "91/91 - 0s - loss: 5675733.3846 - mae: 1853.5759 - val_loss: 13946608.0000 - val_mae: 3234.6731\n",
      "Epoch 202/350\n",
      "91/91 - 0s - loss: 5582671.1868 - mae: 1843.8147 - val_loss: 13867397.0000 - val_mae: 3210.4285\n",
      "Epoch 203/350\n",
      "91/91 - 0s - loss: 5747461.8462 - mae: 1873.0769 - val_loss: 13855120.0000 - val_mae: 3201.6023\n",
      "Epoch 204/350\n",
      "91/91 - 0s - loss: 5568857.7308 - mae: 1840.4049 - val_loss: 13740711.0000 - val_mae: 3280.7129\n",
      "Epoch 205/350\n",
      "91/91 - 0s - loss: 5750654.1154 - mae: 1861.5626 - val_loss: 13668179.0000 - val_mae: 3241.7371\n",
      "Epoch 206/350\n",
      "91/91 - 0s - loss: 5554581.5549 - mae: 1833.0392 - val_loss: 13609783.0000 - val_mae: 3203.5593\n",
      "Epoch 207/350\n",
      "91/91 - 0s - loss: 5614135.7912 - mae: 1848.9945 - val_loss: 13661971.0000 - val_mae: 3207.7571\n",
      "Epoch 208/350\n",
      "91/91 - 0s - loss: 5562372.1703 - mae: 1848.9900 - val_loss: 13886160.0000 - val_mae: 3203.5168\n",
      "Epoch 209/350\n",
      "91/91 - 0s - loss: 5705649.7857 - mae: 1871.9746 - val_loss: 13946611.0000 - val_mae: 3201.7302\n",
      "Epoch 210/350\n",
      "91/91 - 0s - loss: 5757182.3791 - mae: 1855.1060 - val_loss: 14240972.0000 - val_mae: 3364.0535\n",
      "Epoch 211/350\n",
      "91/91 - 0s - loss: 5670368.0659 - mae: 1839.9420 - val_loss: 13932895.0000 - val_mae: 3221.8955\n",
      "Epoch 212/350\n",
      "91/91 - 0s - loss: 5471196.8407 - mae: 1813.6882 - val_loss: 14263296.0000 - val_mae: 3189.9434\n",
      "Epoch 213/350\n",
      "91/91 - 0s - loss: 5783619.7253 - mae: 1885.2369 - val_loss: 13969623.0000 - val_mae: 3222.5918\n",
      "Epoch 214/350\n",
      "91/91 - 0s - loss: 5452406.9396 - mae: 1817.2771 - val_loss: 13964789.0000 - val_mae: 3241.8027\n",
      "Epoch 215/350\n",
      "91/91 - 0s - loss: 5527858.6374 - mae: 1822.8961 - val_loss: 13765168.0000 - val_mae: 3253.0759\n",
      "Epoch 216/350\n",
      "91/91 - 0s - loss: 5520512.4066 - mae: 1826.1772 - val_loss: 13655280.0000 - val_mae: 3198.1199\n",
      "Epoch 217/350\n",
      "91/91 - 0s - loss: 5463856.0797 - mae: 1825.2804 - val_loss: 13766633.0000 - val_mae: 3195.4229\n",
      "Epoch 218/350\n",
      "91/91 - 0s - loss: 5576495.1154 - mae: 1849.3610 - val_loss: 13883149.0000 - val_mae: 3198.5886\n",
      "Epoch 219/350\n",
      "91/91 - 0s - loss: 5376893.6758 - mae: 1802.7429 - val_loss: 14018600.0000 - val_mae: 3297.3567\n",
      "Epoch 220/350\n",
      "91/91 - 0s - loss: 5525168.0000 - mae: 1815.1047 - val_loss: 13826035.0000 - val_mae: 3224.3665\n",
      "Epoch 221/350\n",
      "91/91 - 0s - loss: 5499200.5577 - mae: 1823.5710 - val_loss: 13721637.0000 - val_mae: 3184.7598\n",
      "Epoch 222/350\n",
      "91/91 - 0s - loss: 5442350.2363 - mae: 1812.7500 - val_loss: 13892235.0000 - val_mae: 3206.0576\n",
      "Epoch 223/350\n",
      "91/91 - 0s - loss: 5414347.2170 - mae: 1797.0054 - val_loss: 13869507.0000 - val_mae: 3199.7754\n",
      "Epoch 224/350\n",
      "91/91 - 0s - loss: 5550411.5440 - mae: 1819.5667 - val_loss: 13854579.0000 - val_mae: 3213.2263\n",
      "Epoch 225/350\n",
      "91/91 - 0s - loss: 5227074.2637 - mae: 1785.3555 - val_loss: 13968217.0000 - val_mae: 3179.8362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226/350\n",
      "91/91 - 0s - loss: 5363350.0495 - mae: 1808.4742 - val_loss: 14277639.0000 - val_mae: 3178.7131\n",
      "Epoch 227/350\n",
      "91/91 - 0s - loss: 5308797.5824 - mae: 1794.8905 - val_loss: 13911763.0000 - val_mae: 3238.6262\n",
      "Epoch 228/350\n",
      "91/91 - 0s - loss: 5337695.6978 - mae: 1769.2715 - val_loss: 14143487.0000 - val_mae: 3345.9912\n",
      "Epoch 229/350\n",
      "91/91 - 0s - loss: 5306716.0659 - mae: 1784.7787 - val_loss: 13902000.0000 - val_mae: 3207.0032\n",
      "Epoch 230/350\n",
      "91/91 - 0s - loss: 5117439.4560 - mae: 1772.2551 - val_loss: 14372380.0000 - val_mae: 3171.2058\n",
      "Epoch 231/350\n",
      "91/91 - 0s - loss: 5486212.1703 - mae: 1834.3409 - val_loss: 14410328.0000 - val_mae: 3190.6770\n",
      "Epoch 232/350\n",
      "91/91 - 0s - loss: 5210820.7747 - mae: 1781.6515 - val_loss: 14693256.0000 - val_mae: 3358.9189\n",
      "Epoch 233/350\n",
      "91/91 - 0s - loss: 5457630.3791 - mae: 1797.3329 - val_loss: 14224637.0000 - val_mae: 3303.2888\n",
      "Epoch 234/350\n",
      "91/91 - 0s - loss: 5263897.9066 - mae: 1804.8309 - val_loss: 14251047.0000 - val_mae: 3199.4504\n",
      "Epoch 235/350\n",
      "91/91 - 0s - loss: 5387989.4615 - mae: 1831.7961 - val_loss: 13999777.0000 - val_mae: 3198.8262\n",
      "Epoch 236/350\n",
      "91/91 - 0s - loss: 5160276.7912 - mae: 1772.9590 - val_loss: 14048375.0000 - val_mae: 3278.3066\n",
      "Epoch 237/350\n",
      "91/91 - 0s - loss: 5287866.0165 - mae: 1763.9266 - val_loss: 14207779.0000 - val_mae: 3299.6057\n",
      "Epoch 238/350\n",
      "91/91 - 0s - loss: 5235018.4313 - mae: 1772.7510 - val_loss: 14454688.0000 - val_mae: 3202.6921\n",
      "Epoch 239/350\n",
      "91/91 - 0s - loss: 5186625.2363 - mae: 1779.9464 - val_loss: 14498485.0000 - val_mae: 3212.2673\n",
      "Epoch 240/350\n",
      "91/91 - 0s - loss: 5254384.5110 - mae: 1776.2153 - val_loss: 14163720.0000 - val_mae: 3248.5984\n",
      "Epoch 241/350\n",
      "91/91 - 0s - loss: 5128721.5549 - mae: 1742.0502 - val_loss: 13892571.0000 - val_mae: 3203.6858\n",
      "Epoch 242/350\n",
      "91/91 - 0s - loss: 5164130.2912 - mae: 1762.0400 - val_loss: 14084849.0000 - val_mae: 3208.8757\n",
      "Epoch 243/350\n",
      "91/91 - 0s - loss: 5137160.2363 - mae: 1756.6575 - val_loss: 14113297.0000 - val_mae: 3230.0527\n",
      "Epoch 244/350\n",
      "91/91 - 0s - loss: 5050463.6401 - mae: 1732.2186 - val_loss: 14283101.0000 - val_mae: 3218.2615\n",
      "Epoch 245/350\n",
      "91/91 - 0s - loss: 5063622.5659 - mae: 1737.6874 - val_loss: 14376979.0000 - val_mae: 3207.8242\n",
      "Epoch 246/350\n",
      "91/91 - 0s - loss: 5050224.1538 - mae: 1739.2993 - val_loss: 14276604.0000 - val_mae: 3210.8196\n",
      "Epoch 247/350\n",
      "91/91 - 0s - loss: 5069767.9780 - mae: 1738.2998 - val_loss: 14167007.0000 - val_mae: 3228.5637\n",
      "Epoch 248/350\n",
      "91/91 - 0s - loss: 5044774.1978 - mae: 1727.5524 - val_loss: 14061485.0000 - val_mae: 3219.9932\n",
      "Epoch 249/350\n",
      "91/91 - 0s - loss: 5044052.2857 - mae: 1753.4695 - val_loss: 14190248.0000 - val_mae: 3196.8381\n",
      "Epoch 250/350\n",
      "91/91 - 0s - loss: 5086754.6813 - mae: 1757.0865 - val_loss: 14164360.0000 - val_mae: 3226.7031\n",
      "Epoch 251/350\n",
      "91/91 - 0s - loss: 5065974.4890 - mae: 1744.2938 - val_loss: 14359555.0000 - val_mae: 3220.8972\n",
      "Epoch 252/350\n",
      "91/91 - 0s - loss: 4994409.0275 - mae: 1721.9739 - val_loss: 14378064.0000 - val_mae: 3208.7832\n",
      "Epoch 253/350\n",
      "91/91 - 0s - loss: 4986184.0110 - mae: 1736.8297 - val_loss: 14292652.0000 - val_mae: 3199.0081\n",
      "Epoch 254/350\n",
      "91/91 - 0s - loss: 5015188.6099 - mae: 1736.9443 - val_loss: 14129629.0000 - val_mae: 3217.3933\n",
      "Epoch 255/350\n",
      "91/91 - 0s - loss: 5004169.8901 - mae: 1728.0284 - val_loss: 14036141.0000 - val_mae: 3225.5537\n",
      "Epoch 256/350\n",
      "91/91 - 0s - loss: 5049017.7418 - mae: 1738.1604 - val_loss: 14385291.0000 - val_mae: 3228.4414\n",
      "Epoch 257/350\n",
      "91/91 - 0s - loss: 5005072.3187 - mae: 1730.6093 - val_loss: 14474447.0000 - val_mae: 3214.8845\n",
      "Epoch 258/350\n",
      "91/91 - 0s - loss: 5378560.4835 - mae: 1816.4475 - val_loss: 14860081.0000 - val_mae: 3181.9563\n",
      "Epoch 259/350\n",
      "91/91 - 0s - loss: 5077272.8956 - mae: 1758.6047 - val_loss: 14635997.0000 - val_mae: 3395.4661\n",
      "Epoch 260/350\n",
      "91/91 - 0s - loss: 5166301.1923 - mae: 1726.9092 - val_loss: 14281491.0000 - val_mae: 3253.5735\n",
      "Epoch 261/350\n",
      "91/91 - 0s - loss: 4940494.1703 - mae: 1716.7462 - val_loss: 14467429.0000 - val_mae: 3188.5515\n",
      "Epoch 262/350\n",
      "91/91 - 0s - loss: 5039105.8846 - mae: 1751.6836 - val_loss: 14317628.0000 - val_mae: 3195.3672\n",
      "Epoch 263/350\n",
      "91/91 - 0s - loss: 4902492.7912 - mae: 1723.0941 - val_loss: 14195389.0000 - val_mae: 3268.4629\n",
      "Epoch 264/350\n",
      "91/91 - 0s - loss: 4993457.8516 - mae: 1732.8333 - val_loss: 14243864.0000 - val_mae: 3212.7571\n",
      "Epoch 265/350\n",
      "91/91 - 0s - loss: 4920507.3571 - mae: 1715.4565 - val_loss: 14308032.0000 - val_mae: 3191.4668\n",
      "Epoch 266/350\n",
      "91/91 - 0s - loss: 4956892.8242 - mae: 1731.9459 - val_loss: 14611805.0000 - val_mae: 3162.2395\n",
      "Epoch 267/350\n",
      "91/91 - 0s - loss: 5030030.3846 - mae: 1734.7902 - val_loss: 14251800.0000 - val_mae: 3241.1992\n",
      "Epoch 268/350\n",
      "91/91 - 0s - loss: 4924113.7527 - mae: 1706.4688 - val_loss: 14085664.0000 - val_mae: 3183.9138\n",
      "Epoch 269/350\n",
      "91/91 - 0s - loss: 4849973.2527 - mae: 1704.2301 - val_loss: 13983475.0000 - val_mae: 3204.9180\n",
      "Epoch 270/350\n",
      "91/91 - 0s - loss: 4866237.6648 - mae: 1698.4784 - val_loss: 14115413.0000 - val_mae: 3196.1836\n",
      "Epoch 271/350\n",
      "91/91 - 0s - loss: 4799191.1484 - mae: 1690.5155 - val_loss: 14296301.0000 - val_mae: 3182.6594\n",
      "Epoch 272/350\n",
      "91/91 - 0s - loss: 4843461.2637 - mae: 1719.4342 - val_loss: 14418155.0000 - val_mae: 3169.6248\n",
      "Epoch 273/350\n",
      "91/91 - 0s - loss: 4828470.4725 - mae: 1703.0249 - val_loss: 14462529.0000 - val_mae: 3191.1887\n",
      "Epoch 274/350\n",
      "91/91 - 0s - loss: 4816964.6319 - mae: 1696.5347 - val_loss: 14382809.0000 - val_mae: 3206.5120\n",
      "Epoch 275/350\n",
      "91/91 - 0s - loss: 4753124.5165 - mae: 1692.0730 - val_loss: 14431856.0000 - val_mae: 3216.5085\n",
      "Epoch 276/350\n",
      "91/91 - 0s - loss: 4794929.0907 - mae: 1708.1663 - val_loss: 14383312.0000 - val_mae: 3218.7891\n",
      "Epoch 277/350\n",
      "91/91 - 0s - loss: 4776193.3187 - mae: 1699.3214 - val_loss: 14442453.0000 - val_mae: 3219.6809\n",
      "Epoch 278/350\n",
      "91/91 - 0s - loss: 4807140.7363 - mae: 1688.3534 - val_loss: 14392240.0000 - val_mae: 3199.6609\n",
      "Epoch 279/350\n",
      "91/91 - 0s - loss: 4745001.7637 - mae: 1682.3413 - val_loss: 14424963.0000 - val_mae: 3191.6179\n",
      "Epoch 280/350\n",
      "91/91 - 0s - loss: 4868139.4011 - mae: 1717.5447 - val_loss: 14481195.0000 - val_mae: 3198.7551\n",
      "Epoch 281/350\n",
      "91/91 - 0s - loss: 4702572.2418 - mae: 1692.2491 - val_loss: 14589596.0000 - val_mae: 3167.8894\n",
      "Epoch 282/350\n",
      "91/91 - 0s - loss: 4831312.1538 - mae: 1711.4886 - val_loss: 14258305.0000 - val_mae: 3196.8135\n",
      "Epoch 283/350\n",
      "91/91 - 0s - loss: 4717918.7308 - mae: 1694.3766 - val_loss: 14156405.0000 - val_mae: 3183.1575\n",
      "Epoch 284/350\n",
      "91/91 - 0s - loss: 4716226.9698 - mae: 1692.7809 - val_loss: 14354861.0000 - val_mae: 3192.5723\n",
      "Epoch 285/350\n",
      "91/91 - 0s - loss: 4717240.0852 - mae: 1678.9189 - val_loss: 14379557.0000 - val_mae: 3195.4041\n",
      "Epoch 286/350\n",
      "91/91 - 0s - loss: 4730677.2582 - mae: 1681.5800 - val_loss: 14388573.0000 - val_mae: 3159.9199\n",
      "Epoch 287/350\n",
      "91/91 - 0s - loss: 4903266.8022 - mae: 1714.9718 - val_loss: 13967669.0000 - val_mae: 3182.8821\n",
      "Epoch 288/350\n",
      "91/91 - 0s - loss: 4666308.6923 - mae: 1674.5763 - val_loss: 14216772.0000 - val_mae: 3171.8926\n",
      "Epoch 289/350\n",
      "91/91 - 0s - loss: 4887762.9231 - mae: 1713.5774 - val_loss: 14849776.0000 - val_mae: 3178.1521\n",
      "Epoch 290/350\n",
      "91/91 - 0s - loss: 4874494.6154 - mae: 1686.0752 - val_loss: 14684649.0000 - val_mae: 3276.9277\n",
      "Epoch 291/350\n",
      "91/91 - 0s - loss: 4769227.6209 - mae: 1675.9241 - val_loss: 14378123.0000 - val_mae: 3154.2410\n",
      "Epoch 292/350\n",
      "91/91 - 0s - loss: 4725210.5687 - mae: 1684.9252 - val_loss: 14225641.0000 - val_mae: 3160.5261\n",
      "Epoch 293/350\n",
      "91/91 - 0s - loss: 4649735.0632 - mae: 1692.6010 - val_loss: 14034981.0000 - val_mae: 3203.1074\n",
      "Epoch 294/350\n",
      "91/91 - 0s - loss: 4692291.9011 - mae: 1671.0739 - val_loss: 13877992.0000 - val_mae: 3167.4485\n",
      "Epoch 295/350\n",
      "91/91 - 0s - loss: 4601981.4945 - mae: 1659.4938 - val_loss: 14036560.0000 - val_mae: 3152.8660\n",
      "Epoch 296/350\n",
      "91/91 - 0s - loss: 4615012.0879 - mae: 1657.4403 - val_loss: 14194580.0000 - val_mae: 3132.4075\n",
      "Epoch 297/350\n",
      "91/91 - 0s - loss: 4608568.2857 - mae: 1654.5360 - val_loss: 14089163.0000 - val_mae: 3141.6667\n",
      "Epoch 298/350\n",
      "91/91 - 0s - loss: 4868887.7005 - mae: 1709.5624 - val_loss: 14298539.0000 - val_mae: 3147.5378\n",
      "Epoch 299/350\n",
      "91/91 - 0s - loss: 4541777.0659 - mae: 1647.2159 - val_loss: 14411351.0000 - val_mae: 3242.7053\n",
      "Epoch 300/350\n",
      "91/91 - 0s - loss: 4766246.3352 - mae: 1664.4076 - val_loss: 14366312.0000 - val_mae: 3176.4177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/350\n",
      "91/91 - 0s - loss: 4773346.6703 - mae: 1692.3746 - val_loss: 14838777.0000 - val_mae: 3142.4629\n",
      "Epoch 302/350\n",
      "91/91 - 0s - loss: 4718581.8297 - mae: 1661.4738 - val_loss: 14188524.0000 - val_mae: 3225.0598\n",
      "Epoch 303/350\n",
      "91/91 - 0s - loss: 4774194.5769 - mae: 1652.6267 - val_loss: 14170499.0000 - val_mae: 3209.6067\n",
      "Epoch 304/350\n",
      "91/91 - 0s - loss: 4844954.9286 - mae: 1690.9108 - val_loss: 14789653.0000 - val_mae: 3124.0847\n",
      "Epoch 305/350\n",
      "91/91 - 0s - loss: 4595892.0330 - mae: 1651.1827 - val_loss: 14016360.0000 - val_mae: 3052.1697\n",
      "Epoch 306/350\n",
      "91/91 - 0s - loss: 4881559.7995 - mae: 1702.1078 - val_loss: 13857179.0000 - val_mae: 3068.9714\n",
      "Epoch 307/350\n",
      "91/91 - 0s - loss: 4871951.9560 - mae: 1716.3390 - val_loss: 14456371.0000 - val_mae: 3054.5691\n",
      "Epoch 308/350\n",
      "91/91 - 0s - loss: 4344931.7418 - mae: 1645.5288 - val_loss: 13759013.0000 - val_mae: 3020.3562\n",
      "Epoch 309/350\n",
      "91/91 - 0s - loss: 4153488.9835 - mae: 1587.4100 - val_loss: 13832648.0000 - val_mae: 3026.4358\n",
      "Epoch 310/350\n",
      "91/91 - 0s - loss: 4562552.4231 - mae: 1654.6315 - val_loss: 14040836.0000 - val_mae: 3044.7942\n",
      "Epoch 311/350\n",
      "91/91 - 0s - loss: 4668666.8736 - mae: 1684.4244 - val_loss: 15287661.0000 - val_mae: 3124.2793\n",
      "Epoch 312/350\n",
      "91/91 - 0s - loss: 5405642.3956 - mae: 1734.9229 - val_loss: 11909029.0000 - val_mae: 2685.5674\n",
      "Epoch 313/350\n",
      "91/91 - 0s - loss: 5521846.9945 - mae: 1724.7997 - val_loss: 12569013.0000 - val_mae: 2803.5139\n",
      "Epoch 314/350\n",
      "91/91 - 0s - loss: 5428898.5412 - mae: 1710.1061 - val_loss: 10749953.0000 - val_mae: 2675.7683\n",
      "Epoch 315/350\n",
      "91/91 - 0s - loss: 5472136.1429 - mae: 1737.8253 - val_loss: 10169177.0000 - val_mae: 2580.4221\n",
      "Epoch 316/350\n",
      "91/91 - 0s - loss: 5301731.1209 - mae: 1687.9401 - val_loss: 10692707.0000 - val_mae: 2638.9092\n",
      "Epoch 317/350\n",
      "91/91 - 0s - loss: 5461816.0769 - mae: 1731.0076 - val_loss: 10182329.0000 - val_mae: 2469.1355\n",
      "Epoch 318/350\n",
      "91/91 - 0s - loss: 5162046.9066 - mae: 1687.1807 - val_loss: 10442019.0000 - val_mae: 2592.3499\n",
      "Epoch 319/350\n",
      "91/91 - 0s - loss: 5231548.8022 - mae: 1718.1722 - val_loss: 11699859.0000 - val_mae: 2753.5129\n",
      "Epoch 320/350\n",
      "91/91 - 0s - loss: 5511767.4835 - mae: 1748.0009 - val_loss: 12054496.0000 - val_mae: 2837.8372\n",
      "Epoch 321/350\n",
      "91/91 - 0s - loss: 5614398.7418 - mae: 1755.0100 - val_loss: 11893789.0000 - val_mae: 2843.4412\n",
      "Epoch 322/350\n",
      "91/91 - 0s - loss: 5498242.9231 - mae: 1732.1625 - val_loss: 11305005.0000 - val_mae: 2790.1204\n",
      "Epoch 323/350\n",
      "91/91 - 0s - loss: 5630625.3956 - mae: 1808.0490 - val_loss: 11080359.0000 - val_mae: 2746.9431\n",
      "Epoch 324/350\n",
      "91/91 - 0s - loss: 5510642.9780 - mae: 1761.8878 - val_loss: 11764371.0000 - val_mae: 2744.3359\n",
      "Epoch 325/350\n",
      "91/91 - 0s - loss: 5852908.4505 - mae: 1809.2167 - val_loss: 12527427.0000 - val_mae: 2817.0938\n",
      "Epoch 326/350\n",
      "91/91 - 0s - loss: 5452725.2363 - mae: 1750.8335 - val_loss: 11792844.0000 - val_mae: 2796.9817\n",
      "Epoch 327/350\n",
      "91/91 - 0s - loss: 5415072.3407 - mae: 1767.6671 - val_loss: 11442684.0000 - val_mae: 2768.0613\n",
      "Epoch 328/350\n",
      "91/91 - 0s - loss: 5346799.1264 - mae: 1708.4852 - val_loss: 11536255.0000 - val_mae: 2736.0471\n",
      "Epoch 329/350\n",
      "91/91 - 0s - loss: 5299076.0934 - mae: 1693.1816 - val_loss: 11787112.0000 - val_mae: 2764.6379\n",
      "Epoch 330/350\n",
      "91/91 - 0s - loss: 5166505.1044 - mae: 1677.5347 - val_loss: 12074019.0000 - val_mae: 2787.3762\n",
      "Epoch 331/350\n",
      "91/91 - 0s - loss: 5332810.4341 - mae: 1718.4657 - val_loss: 12033315.0000 - val_mae: 2767.0488\n",
      "Epoch 332/350\n",
      "91/91 - 0s - loss: 5250425.9368 - mae: 1695.9133 - val_loss: 11481368.0000 - val_mae: 2747.9216\n",
      "Epoch 333/350\n",
      "91/91 - 0s - loss: 5271309.2143 - mae: 1697.1212 - val_loss: 11446315.0000 - val_mae: 2756.1758\n",
      "Epoch 334/350\n",
      "91/91 - 0s - loss: 5259523.8352 - mae: 1696.5903 - val_loss: 11806763.0000 - val_mae: 2802.7561\n",
      "Epoch 335/350\n",
      "91/91 - 0s - loss: 5492276.9451 - mae: 1762.4247 - val_loss: 11630301.0000 - val_mae: 2779.8220\n",
      "Epoch 336/350\n",
      "91/91 - 0s - loss: 5145248.9451 - mae: 1714.0862 - val_loss: 11493200.0000 - val_mae: 2765.1609\n",
      "Epoch 337/350\n",
      "91/91 - 0s - loss: 5319612.0604 - mae: 1707.9536 - val_loss: 11311891.0000 - val_mae: 2717.8635\n",
      "Epoch 338/350\n",
      "91/91 - 0s - loss: 5664817.0385 - mae: 1817.9316 - val_loss: 12240509.0000 - val_mae: 2806.1055\n",
      "Epoch 339/350\n",
      "91/91 - 0s - loss: 5458792.2308 - mae: 1732.5873 - val_loss: 12978099.0000 - val_mae: 3023.5051\n",
      "Epoch 340/350\n",
      "91/91 - 0s - loss: 5364094.2115 - mae: 1688.0055 - val_loss: 12001725.0000 - val_mae: 2812.4084\n",
      "Epoch 341/350\n",
      "91/91 - 0s - loss: 5127865.1731 - mae: 1705.1310 - val_loss: 11315093.0000 - val_mae: 2709.2947\n",
      "Epoch 342/350\n",
      "91/91 - 0s - loss: 5144256.6374 - mae: 1702.2360 - val_loss: 11294341.0000 - val_mae: 2708.5554\n",
      "Epoch 343/350\n",
      "91/91 - 0s - loss: 5203157.2143 - mae: 1682.3502 - val_loss: 11653183.0000 - val_mae: 2784.7722\n",
      "Epoch 344/350\n",
      "91/91 - 0s - loss: 5011338.6951 - mae: 1643.6278 - val_loss: 12003501.0000 - val_mae: 2801.6858\n",
      "Epoch 345/350\n",
      "91/91 - 0s - loss: 5038734.5055 - mae: 1670.6567 - val_loss: 12140593.0000 - val_mae: 2781.3098\n",
      "Epoch 346/350\n",
      "91/91 - 0s - loss: 4983089.4093 - mae: 1674.1820 - val_loss: 12066339.0000 - val_mae: 2779.9094\n",
      "Epoch 347/350\n",
      "91/91 - 0s - loss: 4975526.7363 - mae: 1653.2438 - val_loss: 11387223.0000 - val_mae: 2747.7004\n",
      "Epoch 348/350\n",
      "91/91 - 0s - loss: 5015187.0879 - mae: 1671.3298 - val_loss: 11476200.0000 - val_mae: 2783.1133\n",
      "Epoch 349/350\n",
      "91/91 - 0s - loss: 4878323.7143 - mae: 1643.1104 - val_loss: 11748996.0000 - val_mae: 2776.4905\n",
      "Epoch 350/350\n",
      "91/91 - 0s - loss: 4953006.9615 - mae: 1633.3496 - val_loss: 11929657.0000 - val_mae: 2784.9685\n",
      "MSE: 11929657.000, RMSE: 3453.934, MAE: 2784.969\n",
      "Predicted: 17779.953\n"
     ]
    }
   ],
   "source": [
    "# lstm for time series forecasting\n",
    "from numpy import sqrt\n",
    "from numpy import asarray\n",
    "from pandas import read_csv\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return asarray(X), asarray(y)\n",
    "\n",
    "# load the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-car-sales.csv'\n",
    "df = read_csv(path, header=0, index_col=0, squeeze=True)\n",
    "# retrieve the values\n",
    "values = df.values.astype('float32')\n",
    "# specify the window size\n",
    "n_steps = 5\n",
    "# split into samples\n",
    "X, y = split_sequence(values, n_steps)\n",
    "# reshape into [samples, timesteps, features]\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "# split into train/test\n",
    "n_test = 12\n",
    "X_train, X_test, y_train, y_test = X[:-n_test], X[-n_test:], y[:-n_test], y[-n_test:]\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', kernel_initializer='he_normal', input_shape=(n_steps,1)))\n",
    "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=350, batch_size=32, verbose=2, validation_data=(X_test, y_test))\n",
    "# evaluate the model\n",
    "mse, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('MSE: %.3f, RMSE: %.3f, MAE: %.3f' % (mse, sqrt(mse), mae))\n",
    "# make a prediction\n",
    "row = asarray([18024.0, 16722.0, 14385.0, 21342.0, 17180.0]).reshape((1, n_steps, 1))\n",
    "yhat = model.predict(row)\n",
    "print('Predicted: %.3f' % (yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
